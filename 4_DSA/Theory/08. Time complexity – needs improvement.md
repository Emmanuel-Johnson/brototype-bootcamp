# Time Complexity

## 1. What is it?
Time complexity measures how the runtime of an algorithm increases relative to the size of its input (usually denoted as `n`).

## 2. Why it matters?
It helps you evaluate and compare algorithms, especially when dealing with large datasets, to ensure good performance and scalability.

## 3. How it works?
We analyze an algorithm's performance in different scenarios:

- **Best Case (Ω)**: The shortest time the algorithm takes (ideal input).
- **Average Case (Θ)**: The expected time over all inputs.
- **Worst Case (O)**: The maximum time the algorithm might take (most complex input).

These are expressed using mathematical notations like **Big O**, **Big Omega (Ω)**, and **Big Theta (Θ)**.

## 4. Example/Analogy
Think of algorithms like race cars on a track:  
If the track gets longer (larger input), how much longer does the car take to finish?  
A faster car (more efficient algorithm) might only take slightly longer as the track grows (e.g., `O(n)`), while a slower one might take much longer (e.g., `O(n²)`).

## 5. When to use it?
Use it when designing, choosing, or analyzing algorithms to ensure your program performs well, especially for large inputs or time-critical applications.

## 6. Pros and Cons
✅ Helps predict performance as input grows  
❌ Doesn’t account for constants, actual hardware, or input-specific optimizations

## 7. Related Concepts
- **Space Complexity** – how much memory an algorithm uses  
- **Best/Worst/Average Case Analysis**  
- **Asymptotic Notations**: Big O, Ω, Θ

## 8. Conclusion
Time complexity is essential in understanding how scalable and efficient an algorithm is.  
It’s a key concept in algorithm design and competitive programming.

---
